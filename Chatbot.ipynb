{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPIxHtK9iATr7PfFZbxJSst",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moushumipriya/Norwegian--Chatbot/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16tRTtNp_0FJ"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install Dependencies and Download Llama Model\n",
        "\n",
        "# Install all necessary libraries\n",
        "!pip install numpy==1.26.4\n",
        "!pip install faiss-cpu\n",
        "!pip install transformers sentence-transformers PyPDF2 sentencepiece accelerate\n",
        "!pip install llama-cpp-python\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create necessary folders\n",
        "if not os.path.exists(\"./models\"):\n",
        "    os.makedirs(\"./models\")\n",
        "if not os.path.exists(\"./embeddings\"):\n",
        "    os.makedirs(\"./embeddings\")\n",
        "\n",
        "# Define the model download URL\n",
        "model_url = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf?download=true\"\n",
        "model_filename = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
        "\n",
        "print(f\"Downloading Llama model from the link...\")\n",
        "# Use wget to download the model directly to the models folder\n",
        "!wget -O ./models/{model_filename} \"{model_url}\"\n",
        "\n",
        "print(\"Llama model download complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Upload PDF File\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Please upload your PDF file:\")\n",
        "uploaded_pdf = files.upload()\n",
        "\n",
        "if uploaded_pdf:\n",
        "    pdf_filename = list(uploaded_pdf.keys())[0]\n",
        "    print(f\"File '{pdf_filename}' uploaded successfully.\")\n",
        "\n",
        "    if pdf_filename != 'data.pdf':\n",
        "        os.rename(pdf_filename, 'data.pdf')\n",
        "        print(f\"File has been renamed to 'data.pdf'.\")\n",
        "else:\n",
        "    print(\"Error: No file was uploaded. Please try again.\")\n",
        "\n",
        "if os.path.exists('data.pdf'):\n",
        "    print(\"Verification successful: 'data.pdf' is in the current directory.\")\n",
        "else:\n",
        "    print(\"Verification failed: 'data.pdf' could not be found after upload.\")"
      ],
      "metadata": {
        "id": "1zJBKDR_ABsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D3EeP4mWAC9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Main Chatbot Functions and Configuration\n",
        "\n",
        "import PyPDF2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from llama_cpp import Llama\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "PROJECT_ROOT = \"./\"\n",
        "PDF_FILE_PATH = os.path.join(PROJECT_ROOT, \"data.pdf\")\n",
        "FAISS_INDEX_PATH = os.path.join(PROJECT_ROOT, \"embeddings/faiss_index.pkl\")\n",
        "LLM_MODEL_PATH = os.path.join(PROJECT_ROOT, \"models\", \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\")\n",
        "\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 50\n",
        "TOP_K_RETRIEVAL = 5\n",
        "LLM_N_GPU_LAYERS = -1\n",
        "LLM_N_CTX = 4096\n",
        "LLM_TEMPERATURE = 0.7\n",
        "\n",
        "# --- Global Models ---\n",
        "embedding_model = None\n",
        "tokenizer = None\n",
        "translator_en_to_no = None\n",
        "translator_no_to_en = None\n",
        "translator_en_to_bn = None\n",
        "translator_bn_to_en = None\n",
        "translator_no_to_bn = None\n",
        "translator_bn_to_no = None\n",
        "llm_model = None\n",
        "\n",
        "# --- Text Processing and Embedding Functions ---\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                page = reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "        print(f\"Extracted {len(text)} characters from PDF.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF '{pdf_path}': {e}\")\n",
        "        text = \"\"\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap_size=CHUNK_OVERLAP):\n",
        "    if not text or not tokenizer: return []\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    chunks = []\n",
        "    if chunk_size <= overlap_size: chunk_size = overlap_size + 100\n",
        "    for i in range(0, len(tokens), chunk_size - overlap_size):\n",
        "        chunk_tokens = tokens[i : i + chunk_size]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens)\n",
        "        chunks.append(chunk_text)\n",
        "    print(f\"Chunked text into {len(chunks)} chunks.\")\n",
        "    return chunks\n",
        "\n",
        "def generate_embeddings(text_chunks):\n",
        "    if not embedding_model or not text_chunks: return []\n",
        "    print(f\"Generating embeddings for {len(text_chunks)} chunks...\")\n",
        "    embeddings = embedding_model.encode(text_chunks, convert_to_tensor=False)\n",
        "    print(\"Embeddings generated.\")\n",
        "    return embeddings.astype('float32')\n",
        "\n",
        "def create_faiss_index(embeddings, chunks, index_path=FAISS_INDEX_PATH):\n",
        "    if not embeddings.tolist(): return\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    os.makedirs(os.path.dirname(index_path), exist_ok=True)\n",
        "    with open(index_path, \"wb\") as f:\n",
        "        pickle.dump({\"index\": index, \"chunks\": chunks}, f)\n",
        "    print(f\"FAISS index created and saved to {index_path}.\")\n",
        "\n",
        "def load_faiss_index(index_path=FAISS_INDEX_PATH):\n",
        "    try:\n",
        "        with open(index_path, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "            index = data[\"index\"]\n",
        "            chunks = data[\"chunks\"]\n",
        "        print(f\"FAISS index loaded from {index_path}.\")\n",
        "        return index, chunks\n",
        "    except FileNotFoundError:\n",
        "        print(f\"FAISS index not found at {index_path}.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading FAISS index: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def search_faiss_index(query_embedding, index, chunks, k=TOP_K_RETRIEVAL):\n",
        "    if index is None or chunks is None or query_embedding is None: return []\n",
        "    query_embedding_np = np.array([query_embedding]).astype('float32')\n",
        "    D, I = index.search(query_embedding_np, k)\n",
        "    retrieved_chunks = [chunks[i] for i in I[0] if i < len(chunks)]\n",
        "    print(f\"Retrieved {len(retrieved_chunks)} chunks.\")\n",
        "    return retrieved_chunks\n",
        "\n",
        "\n",
        "def load_translation_models():\n",
        "    global translator_en_to_no, translator_no_to_en, translator_en_to_bn, translator_bn_to_en, translator_no_to_bn, translator_bn_to_no\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "    print(f\"Loading translation models on {device}...\")\n",
        "\n",
        "    try:\n",
        "        translator_en_to_no = pipeline(\"translation\", model=model_name, src_lang=\"eng_Latn\", tgt_lang=\"nob_Latn\", device=0 if device == \"cuda\" else -1)\n",
        "        translator_no_to_en = pipeline(\"translation\", model=model_name, src_lang=\"nob_Latn\", tgt_lang=\"eng_Latn\", device=0 if device == \"cuda\" else -1)\n",
        "        translator_en_to_bn = pipeline(\"translation\", model=model_name, src_lang=\"eng_Latn\", tgt_lang=\"ben_Beng\", device=0 if device == \"cuda\" else -1)\n",
        "        translator_bn_to_en = pipeline(\"translation\", model=model_name, src_lang=\"ben_Beng\", tgt_lang=\"eng_Latn\", device=0 if device == \"cuda\" else -1)\n",
        "        translator_no_to_bn = pipeline(\"translation\", model=model_name, src_lang=\"nob_Latn\", tgt_lang=\"ben_Beng\", device=0 if device == \"cuda\" else -1)\n",
        "        translator_bn_to_no = pipeline(\"translation\", model=model_name, src_lang=\"ben_Beng\", tgt_lang=\"nob_Latn\", device=0 if device == \"cuda\" else -1)\n",
        "        print(\"All translation models loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not load some translation models. {e}\")\n",
        "        translator_en_to_no = None\n",
        "        translator_no_to_en = None\n",
        "        translator_en_to_bn = None\n",
        "        translator_bn_to_en = None\n",
        "        translator_no_to_bn = None\n",
        "        translator_bn_to_no = None\n",
        "\n",
        "def get_translator(src_lang, tgt_lang):\n",
        "    if src_lang == \"bn\" and tgt_lang == \"en\": return translator_bn_to_en\n",
        "    if src_lang == \"en\" and tgt_lang == \"bn\": return translator_en_to_bn\n",
        "    if src_lang == \"no\" and tgt_lang == \"en\": return translator_no_to_en\n",
        "    if src_lang == \"en\" and tgt_lang == \"no\": return translator_en_to_no\n",
        "    if src_lang == \"bn\" and tgt_lang == \"no\": return translator_bn_to_no\n",
        "    if src_lang == \"no\" and tgt_lang == \"bn\": return translator_no_to_bn\n",
        "    return None\n",
        "\n",
        "def translate(text, src_lang, tgt_lang):\n",
        "    if not text: return \"\"\n",
        "    if src_lang == tgt_lang: return text\n",
        "\n",
        "    translator = get_translator(src_lang, tgt_lang)\n",
        "    if translator:\n",
        "        try:\n",
        "            return translator(text, src_lang=get_lang_code(src_lang), tgt_lang=get_lang_code(tgt_lang))[0]['translation_text']\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating from {src_lang} to {tgt_lang}: {e}\")\n",
        "            return text\n",
        "    else:\n",
        "        print(f\"Translator from {src_lang} to {tgt_lang} not loaded.\")\n",
        "        return text\n",
        "\n",
        "def get_lang_code(lang):\n",
        "    if lang == \"en\": return \"eng_Latn\"\n",
        "    if lang == \"bn\": return \"ben_Beng\"\n",
        "    if lang == \"no\": return \"nob_Latn\"\n",
        "    return \"eng_Latn\"\n",
        "\n",
        "def load_llama_model():\n",
        "    global llm_model\n",
        "    if not os.path.exists(LLM_MODEL_PATH):\n",
        "        print(f\"Error: Llama model not found at {LLM_MODEL_PATH}. Please ensure it's uploaded.\")\n",
        "        return None\n",
        "    try:\n",
        "        print(f\"Loading Llama model from {LLM_MODEL_PATH}...\")\n",
        "        llm_model = Llama(\n",
        "            model_path=LLM_MODEL_PATH,\n",
        "            n_gpu_layers=LLM_N_GPU_LAYERS,\n",
        "            n_ctx=LLM_N_CTX,\n",
        "            verbose=False\n",
        "        )\n",
        "        print(\"Llama model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Llama model: {e}\")\n",
        "        llm_model = None\n",
        "\n",
        "def get_llm_response(prompt):\n",
        "    if llm_model is None:\n",
        "        return \"Sorry, the language model is not available.\"\n",
        "    try:\n",
        "        output = llm_model.create_completion(\n",
        "            prompt,\n",
        "            max_tokens=500,\n",
        "            temperature=LLM_TEMPERATURE,\n",
        "            stop=[\"\\nSpørsmål:\", \"\\nContext:\", \"\\n\"],\n",
        "        )\n",
        "        return output[\"choices\"][0][\"text\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting LLM response: {e}\")\n",
        "        return \"Sorry, there was an error processing your request with the language model.\"\n",
        "\n",
        "def chat_with_bot(query, input_lang):\n",
        "    global faiss_index, text_chunks\n",
        "\n",
        "    translated_query_en = translate(query, input_lang, \"en\")\n",
        "    print(f\"Translated query to EN for retrieval: {translated_query_en}\")\n",
        "\n",
        "    query_embedding_en = generate_embeddings([translated_query_en])\n",
        "    if not query_embedding_en.tolist():\n",
        "        return \"Sorry, I could not process your query.\", \"দুঃখিত, আমি আপনার প্রশ্নটি প্রক্রিয়া করতে পারিনি।\", \"Beklager, jeg kunne ikke behandle spørsmålet ditt.\"\n",
        "\n",
        "    retrieved_chunks = search_faiss_index(query_embedding_en[0], faiss_index, text_chunks)\n",
        "    context = \"\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "    if not context:\n",
        "        return \"I could not find relevant information in the document.\", \"নথিতে আমি প্রাসঙ্গিক তথ্য খুঁজে পাইনি।\", \"Jeg fant ingen relevant informasjon i dokumentet.\"\n",
        "\n",
        "    llm_prompt_en = f\"\"\"You are a helpful assistant that answers questions based on the provided information.\n",
        "    Use only the provided context to answer the question. If the answer is not in the context, say that you do not know.\n",
        "    Provide a concise and direct answer.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {translated_query_en}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    llm_answer_en = get_llm_response(llm_prompt_en)\n",
        "    print(f\"LLM Answer (EN):\\n{llm_answer_en}\")\n",
        "\n",
        "    llm_answer_bn = translate(llm_answer_en, \"en\", \"bn\")\n",
        "    llm_answer_no = translate(llm_answer_en, \"en\", \"no\")\n",
        "\n",
        "    print(f\"LLM Answer (BN):\\n{llm_answer_bn}\")\n",
        "    print(f\"LLM Answer (NO):\\n{llm_answer_no}\")\n",
        "\n",
        "    return llm_answer_en, llm_answer_bn, llm_answer_no"
      ],
      "metadata": {
        "id": "9Kj_MtiAmx8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VCd6lKg6AHt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 (New): Chatbot Initialization\n",
        "\n",
        "def main_init():\n",
        "    global embedding_model, tokenizer, faiss_index, text_chunks\n",
        "\n",
        "    # 1. Load Embedding model and Tokenizer\n",
        "    print(\"Loading embedding model and tokenizer...\")\n",
        "    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "    print(\"Embedding model and tokenizer loaded.\")\n",
        "\n",
        "    # 2. Load Translation models\n",
        "    load_translation_models()\n",
        "\n",
        "    # 3. Load Llama model\n",
        "    load_llama_model()\n",
        "\n",
        "    # 4. Prepare document embeddings\n",
        "    faiss_index, text_chunks = load_faiss_index(FAISS_INDEX_PATH)\n",
        "    if faiss_index is None or text_chunks is None:\n",
        "        print(\"\\nFAISS index not found. Generating new one...\")\n",
        "        raw_text = extract_text_from_pdf(PDF_FILE_PATH)\n",
        "        if raw_text:\n",
        "            chunks = chunk_text(raw_text)\n",
        "            embeddings = generate_embeddings(chunks)\n",
        "            if embeddings.tolist():\n",
        "                create_faiss_index(embeddings, chunks, FAISS_INDEX_PATH)\n",
        "                faiss_index, text_chunks = load_faiss_index(FAISS_INDEX_PATH)\n",
        "            else:\n",
        "                print(\"Failed to generate embeddings. Chatbot will not function.\")\n",
        "                return\n",
        "        else:\n",
        "            print(\"Failed to extract text from PDF. Chatbot will not function.\")\n",
        "            return\n",
        "\n",
        "    print(\"\\nInitialization complete! Chatbot is ready.\")\n",
        "\n",
        "# Run the initialization function\n",
        "main_init()"
      ],
      "metadata": {
        "id": "q6gUHF2JmyAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OPeNYdPsATsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 (New): Input-Output Loop (Revised)\n",
        "\n",
        "# Start the chat loop\n",
        "while True:\n",
        "    user_query = input(\"\\nSelect your language (English/Bangla/Norwegian) or type (exit) to quit: \").lower()\n",
        "    if user_query == \"exit\":\n",
        "        print(\"Chatbot is shutting down.\")\n",
        "        break\n",
        "\n",
        "    input_lang = user_query\n",
        "    user_query = input(\"Enter your question: \")\n",
        "\n",
        "    if input_lang in [\"bn\", \"en\", \"no\"]:\n",
        "        answer_en, answer_bn, answer_no = chat_with_bot(user_query, input_lang)\n",
        "        print(\"\\n--- Chatbot Response ---\")\n",
        "        print(f\"Original Query ({input_lang}): {user_query}\")\n",
        "        print(f\"Answer (English): {answer_en}\")\n",
        "        print(f\"Answer (Bengali): {answer_bn}\")\n",
        "        print(f\"Answer (Norwegian): {answer_no}\")\n",
        "    else:\n",
        "        print(\"Invalid input. Please use bn, en, or no.\")"
      ],
      "metadata": {
        "id": "HIk7RWvumyFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}